# ğŸ“š E-Book Recommendation System

## ğŸ¯ Goal

Build a full-stack, data-powered E-Book Recommendation System that:

* Suggests books based on user reading history and preferences
* Analyzes interaction trends
* Uses a hybrid of web technologies and big data tools

---

## ğŸ› ï¸ Tech Stack Overview

| Layer      | Tools Used                                                                                                                  |
| ---------- | --------------------------------------------------------------------------------------------------------------------------- |
| Frontend   | Next.js, React, Context API                                                                                                 |
| Backend    | Node.js, Express, AuthContext                                                                                               |
| Database   | MongoDB                                                                                                                     |
| Big Data   | Hadoop (HDFS, MapReduce), Hive, Spark MLlib                                                                                 |
| Scheduling | Cron Jobs (node-cron), potentially Apache Airflow (for production scale)                                                    |

---

## ğŸ” Core Functionalities

* **Store interactions in MongoDB** (bookmarks, likes, reads, dislikes)
* **Export logs to HDFS** regularly
* **Process metadata via MapReduce** to count actions per book/genre/author
* **Query insights using Hive** (e.g., total likes per genre)
* **Train collaborative filtering model using Spark MLlib (ALS)**
* **Recommend books to users based on similar behavior**
* **Trend detection via Spark** â†’ push trending books to MongoDB â†’ display on frontend

---

## ğŸ§© Backend Architecture Breakdown

### ğŸ”„ MongoDB Schema Example

```js
interactions: [
  {
    userId: String,
    bookId: String,
    action: 'bookmark' | 'like' | 'dislike' | 'read',
    timestamp: Date
  }
]
```

* Ideal for local user queries
* Easily exportable to CSV/JSON for Hadoop ingestion

### ğŸšš MongoDB â†’ HDFS Export Flow

Batch job (Node.js/Python/cron):

1. Query MongoDB interactions
2. Dump to `interactions.csv`
3. Push to HDFS

### ğŸ› ï¸ MapReduce Jobs

Examples:

* Count total likes/bookmarks per genre
* Aggregate actions per user/author
* Clean and normalize noisy records

### ğŸ Hive Queries

```sql
SELECT genre, COUNT(*) FROM interactions WHERE action = 'like' GROUP BY genre;
```

* Create external tables on HDFS dumps
* Query logs to generate insights

### ğŸ”® Spark MLlib Recommendations

* Input format: `(userId, bookId, action)`
* Use ALS (Alternating Least Squares)
* Output: Personalized recommendations

### ğŸ“ˆ Web Analytics

* Identify trending books
* Push trends to MongoDB for UI rendering
* Optionally use Google Analytics or in-house Spark-based analytics

---

## ğŸŒ Frontend Design: Page Layouts

### ğŸ  `index.tsx`

* **Recommended for You** â†’ based on `preferredGenres` (initially via MongoDB, later via Spark)
* **Explore More** â†’ all books outside preferred genres

### ğŸ§  Recommendation API

Route: `/api/books/recommendations`

* Pulls from MongoDB or post-Spark model output

### ğŸ§­ Auth Context

File: `pages/_app.tsx`

```tsx
import type { AppProps } from 'next/app';
import { AuthProvider } from '@/context/AuthContext';
import '../styles/globals.css';

export default function MyApp({ Component, pageProps }: AppProps) {
  return (
    <AuthProvider>
      <Component {...pageProps} />
    </AuthProvider>
  );
}
```

* Enables `useAuth()` across the app

---

## â±ï¸ Automation Possibilities

### âœ… Theoretically Possible

* Use `node-cron` or Python scheduler
* Export logs hourly
* Trigger MapReduce/Hive/Spark scripts

### âŒ Practically Limited on Localhost

Running all services locally (MongoDB, Hadoop, Spark, Hive, Next.js, Express) will likely exhaust your laptop.

### âœ… Future Deployment Stack

* Automate ETL with: Apache Airflow, AWS Glue, or Databricks Workflows

---

## ğŸ¤” Questions to Finalize Workflow

Answer these to lock the structure:

1. Are you running Hadoop & Spark locally, cluster, or cloud?
2. Are recs real-time or periodic (e.g., daily)?
3. Is user login/registration completed?
4. Does frontend need to show analytics?
5. Is book metadata (genre, author, etc.) already stored?

---

## ğŸš€ Current Status

* âœ… MongoDB schema and local interaction logging
* âœ… AuthContext and frontend structure fixed
* ğŸ› ï¸ Big data pipeline under construction
* ğŸ”œ Spark + Hive integration for recommendations

Keep hitting me with the next file â€” weâ€™re stacking this README like a fortress of logic and code. ğŸ§±âš¡
